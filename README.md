# Case TÃ©cnico Data Engineer - iFood

![iFood](https://img.shields.io/badge/iFood-Data_Architecture-EA1D2C)

## ğŸ“‹ Sobre o Projeto

Este projeto apresenta uma soluÃ§Ã£o completa para o desafio de Data Architecture do iFood, abordando a ingestÃ£o, processamento, armazenamento e anÃ¡lise de dados de corridas de tÃ¡xi de Nova York.

A soluÃ§Ã£o implementa uma arquitetura de dados moderna, escalÃ¡vel e eficiente, utilizando tecnologias como Google Cloud Storage, Delta Lake, PySpark e BigQuery para criar um pipeline de dados que vai desde a ingestÃ£o dos dados brutos atÃ© a disponibilizaÃ§Ã£o para anÃ¡lise.

## ğŸ—ï¸ Arquitetura da SoluÃ§Ã£o

A soluÃ§Ã£o segue uma arquitetura em camadas:

1. **Landing Zone (Raw)**: Armazenamento inicial dos dados brutos em seu formato original (Parquet)
2. **Processing Zone (Delta Lake)**: ConversÃ£o para formato Delta Lake, otimizado para processamento
3. **Consumption Zone (BigQuery)**: DisponibilizaÃ§Ã£o dos dados tratados para anÃ¡lise via SQL

```
Fonte de Dados â†’ GCS (Raw) â†’ Delta Lake â†’ BigQuery â†’ AnÃ¡lises SQL
```

## ğŸ—‚ï¸ Estrutura do Projeto

```
ifood-case/
â”œâ”€ src/                       # CÃ³digo fonte da soluÃ§Ã£o
â”‚  â”œâ”€ to_cloud_storage.ipynb  # Script de ingestÃ£o para o Data Lake
â”‚  â””â”€ lake_to_warehouse.ipynb # Script de processamento para o Data Warehouse
â”œâ”€ analysis/                  # Scripts SQL com as respostas das perguntas
â”‚  â”œâ”€ yellow - mÃ©dia do total_amount por mÃªs para yellow tÃ¡xis.sql
â”‚  â”œâ”€ yellow - mÃ©dia de passageiros por hora.sql
â”‚  â”œâ”€ yellow - anÃ¡lise por dia da semana e hora do dia.sql
â”‚  â””â”€ ComparaÃ§Ã£o entre Yellow e Green tÃ¡xis.sql
â”œâ”€ README.md                  # DocumentaÃ§Ã£o do projeto
â””â”€ requirements.txt           # DependÃªncias do projeto
```

## ğŸ”§ Tecnologias Utilizadas

- **Linguagens**: Python, SQL
- **Framework de Processamento**: PySpark
- **Armazenamento**: Google Cloud Storage (GCS)
- **Formato de Dados**: Parquet, Delta Lake
- **Data Warehouse**: BigQuery
- **Ambiente**: Dataproc Spark Connect

## âš™ï¸ Requisitos

- Conta Google Cloud Platform com acesso a GCS, BigQuery e Dataproc
- Python 3.9+
- Databricks Community Edition ou ambiente Spark local (opcional para testes)

## ğŸš€ InstruÃ§Ãµes de ExecuÃ§Ã£o

### 1. ConfiguraÃ§Ã£o do Ambiente

```bash
# Clonar o repositÃ³rio
git clone https://github.com/seu-usuario/ifood-case.git
cd ifood-case

# Instalar dependÃªncias
pip install -r requirements.txt

# Configurar credenciais GCP (substitua pelo caminho do seu arquivo de credenciais)
export GOOGLE_APPLICATION_CREDENTIALS="/caminho/para/seu/arquivo-credenciais.json"
```

### 2. ExecuÃ§Ã£o dos Notebooks

Os notebooks devem ser executados na seguinte ordem:

1. `src/to_cloud_storage.ipynb` - Realiza a ingestÃ£o dos dados para o Data Lake
2. `src/lake_to_warehouse.ipynb` - Processa os dados e disponibiliza no BigQuery

Estes notebooks podem ser executados em um ambiente Jupyter local, no Databricks Community Edition, ou no Google Colab conectado ao Dataproc.

### 3. AnÃ¡lises SQL

ApÃ³s a execuÃ§Ã£o dos notebooks, as anÃ¡lises SQL podem ser executadas no BigQuery console ou em qualquer ferramenta SQL conectada ao BigQuery.

## ğŸ“Š AnÃ¡lises Realizadas

1. **MÃ©dia do valor total por mÃªs para yellow tÃ¡xis**
   - Analisa a mÃ©dia do `total_amount` recebido em cada mÃªs, considerando todos os yellow tÃ¡xis da frota.

2. **MÃ©dia de passageiros por hora em maio**
   - Calcula a mÃ©dia de `passenger_count` por cada hora do dia no mÃªs de maio de 2023.

3. **AnÃ¡lise por dia da semana e hora do dia**
   - Analisa padrÃµes de uso dos tÃ¡xis por dia da semana e hora do dia.

4. **ComparaÃ§Ã£o entre tÃ¡xis Yellow e Green**
   - Compara mÃ©tricas-chave entre os dois tipos de tÃ¡xis.

## ğŸ“ Justificativas TÃ©cnicas

### Escolha das Tecnologias

- **PySpark**: Processamento distribuÃ­do para lidar com grandes volumes de dados.
- **Delta Lake**: Formato otimizado que oferece ACID transactions, viabilizando operaÃ§Ãµes confiÃ¡veis.
- **Google Cloud Storage**: Armazenamento escalÃ¡vel e de baixo custo para a landing zone.
- **BigQuery**: Data warehouse serverless que permite consultas SQL rÃ¡pidas e eficientes.

### Modelagem de Dados

Optei por uma modelagem simplificada que mantÃ©m as colunas essenciais solicitadas:
- VendorID
- passenger_count
- total_amount
- tpep_pickup_datetime
- tpep_dropoff_datetime

### EstratÃ©gia de IngestÃ£o

Implementei uma estratÃ©gia de ingestÃ£o em duas etapas:
1. IngestÃ£o inicial para GCS (dados brutos)
2. Processamento e disponibilizaÃ§Ã£o em BigQuery (dados tratados)

Esta abordagem permite manter os dados originais intactos enquanto oferece uma camada otimizada para consultas analÃ­ticas.
